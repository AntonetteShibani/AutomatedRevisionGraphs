{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-stage revision graph\n",
    "A network graph model to study the stages of revisions made from one draft to another at sentence level.\n",
    "\n",
    "**Copyright (C) 2020 Antonette Shibani.** Available under the Creative Commons Attribution 3.0 Unported License (https://creativecommons.org/licenses/by/3.0/).\n",
    "\n",
    "Cite work as: _Antonette Shibani (2020) Constructing Automated Revision Graphs: A novel visualization technique to study student writing. In proceedings of the 21th International Conference on Artificial Intelligence in Education._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input files:\n",
    "\"origtext.html\", \"Sample_Draft_1.html\",\"Sample_Draft_2.html\",\"Sample_Draft_3.html\",\"Sample_Draft_4.html\"\n",
    "\n",
    "#### Output file:\n",
    "\"MultistageRevisionGraph.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML  # Allows us to create annotated text using HTML and CSS\n",
    "import tabulate as tb\n",
    "from IPython.display import Image\n",
    "import csv\n",
    "try:\n",
    "    # Python 2\n",
    "    from itertools import izip\n",
    "except ImportError:\n",
    "    # Python 3\n",
    "    izip = zip\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import holoviews as hv \n",
    "import networkx as nx\n",
    "hv.extension('bokeh')\n",
    "#Interactive Graphs: http://holoviews.org/user_guide/Network_Graphs.html\n",
    "\n",
    "%opts Graph [width=400 height=400] \n",
    "\n",
    "import json                                 # We need to be able to work whith JSON\n",
    "from urllib import request, response        # To create requests to TAP and handle responses from TAP API\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions required for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate cosine similarity between two texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def calc_cosine(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to clean the text by removing characters leading to wrong sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleantext(txt):\n",
    "    txt.lower()\n",
    "    txt.replace('?.', '?')\n",
    "    txt.replace('\\xa0', '\\n\\n')\n",
    "    txt = re.sub(r'\\.+', \".\", txt) #regular expression to replace more than one full stop with one full stop\n",
    "    return txt   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAP setup to get analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tapUrl = \"https://tap.utscic.edu.au/\"       # TAP URL\n",
    "endpoint = \"graphql\"                        # The query endpoint on TAP\n",
    "completeUrl = tapUrl + endpoint             # The complete url that the request is posted to\n",
    "#Note: For the open-source version of Text Analytics Pipeline (TAP) API, see https://github.com/heta-io/tap\n",
    "\n",
    "#Connect to TAP to get metrics in the form of json output\n",
    "def getJsonFromTAP(query,text,url):\n",
    "    variables = {'text': text}\n",
    "    escapedQuery = query.replace(\"\\n\", \"\\\\n\") #query.encode('utf8').decode('unicode_escape')\n",
    "    fullQuery = json.dumps({'query': escapedQuery, 'variables': variables})\n",
    "    jsonHeader = {'Content-Type':'application/json'}\n",
    "    tapReq = request.Request(url, data = fullQuery.encode('utf8'), headers = jsonHeader)\n",
    "    tapResponse = \"\"\n",
    "    try:\n",
    "        tapResponse = request.urlopen(tapReq)\n",
    "        body = tapResponse.read().decode('utf8')           \n",
    "        return json.loads(body)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return json.dumps({})\n",
    "    \n",
    "#Get sentences of text from TAP\n",
    "def markupMoveSentences(para):\n",
    "    #Get all sentences from the given text\n",
    "    sentencesQuery = \"query Sentences($text: String!){ annotations(text: $text) { analytics {original} }}\"\n",
    "    jsonData = getJsonFromTAP(sentencesQuery,para,completeUrl)\n",
    "    sentencesJson = jsonData.get('data').get('annotations').get('analytics')\n",
    "    def getSentence(json):\n",
    "        return (json.get('original'))\n",
    "    sentences = list(map(getSentence,sentencesJson)) #get original sentences from text\n",
    "    return sentences\n",
    "\n",
    "#Rhetorical moves parsing from TAP\n",
    "def findMoves(rawtext):\n",
    "    movesQuery =  \"query RhetoricalMoves($text: String) { moves(text:$text,parameters:\\\"{\\\\\\\"grammar\\\\\\\":\\\\\\\"analytic\\\\\\\"}\\\") {analytics}}\"\n",
    "    jsonData = getJsonFromTAP(movesQuery,rawtext,completeUrl)\n",
    "    moves = jsonData.get('data').get('moves').get('analytics')\n",
    "    return(moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create the list of rhetorical move labels for each sentence to add to nodes csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createlabels(moves):\n",
    "    rhetmoveslist = ['emph','contribution','novstat','contrast','tempstat','Surprise','nostat','grow','attitude']\n",
    "    movecount = []\n",
    "    label = []\n",
    "    for i in moves:\n",
    "        if len(i)==0:\n",
    "            label.append('Zero')\n",
    "        else:\n",
    "            #Calculate intersection of moves with the given list of valid moves\n",
    "            intmoves = list(set(i) & set(rhetmoveslist))\n",
    "            if len(intmoves)==1:\n",
    "                label.append('One')\n",
    "            if len(intmoves)==2:\n",
    "                label.append('Two')\n",
    "            if len(intmoves)>2:\n",
    "                label.append('More')\n",
    "    return(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create the NODES csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nodes(origsentenceslist, revsentenceslist, labelmoves):\n",
    "    #Creating the combined list of all sentences for the csv\n",
    "    sentencelist = origsentenceslist+revsentenceslist\n",
    "    \n",
    "    #create-list-with-numbers-between-2-values\n",
    "    index1 = range(1,len(origsentenceslist)+1)\n",
    "    index2 = range(1,len(revsentenceslist)+1)\n",
    "\n",
    "    #Adding s to indicate sentences in index like s1, s2..\n",
    "    #Adding identifier for two texts' sentences t1s1, t1s2, t2s1..\n",
    "    mystring1 = \"t1s\" #appending-the-same-string-to-a-list-of-strings-in-python\n",
    "    mystring2 = \"t2s\"\n",
    "    index1 = [mystring1 + str(x) for x in index1]\n",
    "    index2 = [mystring2 + str(x) for x in index2]\n",
    "\n",
    "    #Creating the combined list of all indices for the csv\n",
    "    indexlist = index1 + index2\n",
    "\n",
    "    x1 = [0.1] * len(origsentenceslist)\n",
    "    x2 = [0.2] * len(revsentenceslist) #Changed 0.5 to 0.2 for multi\n",
    "    #Creating the combined list of all x-positions for the csv\n",
    "    x=x1+x2\n",
    "    #print(x)\n",
    "\n",
    "    #creating a numpy array of numbers to allow decimal points\n",
    "    ylist = np.arange(0.95, 0, -0.05) \n",
    "    #Trimming the lists based on number of sentences\n",
    "    y1 = ylist[:len(origsentenceslist)]\n",
    "    y2 = ylist[:len(revsentenceslist)]\n",
    "\n",
    "    #Creating the combined list of all x-positions for the csv \n",
    "    #Converting numpy arrays to python lists to create the y list\n",
    "    y = y1.tolist() + y2.tolist()\n",
    "    y = [ '%.2f' % elem for elem in y ] #to convert to 2 decimal points\n",
    "    #print(y)\n",
    "    \n",
    "    #write-data-from-two-lists-into-columns-in-a-csv\n",
    "    with open('data/mynodesfinal.csv', 'w', newline='') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(['x','y','index','sentence','label'])\n",
    "        w.writerows(zip(x,y,indexlist,sentencelist,labelmoves))\n",
    "    print(\"Nodes csv created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nodes_more(nodedf, newrevsentenceslist, labelmoves, icount):\n",
    "        \n",
    "    if(icount==3):\n",
    "        mult = [0.3]\n",
    "        mystring = 't3s'\n",
    "    elif(icount==4):\n",
    "        mult = [0.4]\n",
    "        mystring = 't4s'\n",
    "    elif(icount==5):\n",
    "        mult = [0.5]\n",
    "        mystring = 't5s'\n",
    "    elif(icount==6):\n",
    "        mult = [0.6]\n",
    "        mystring = 't6s'\n",
    "    elif(icount==7):\n",
    "        mult = [0.7]\n",
    "        mystring = 't7s'\n",
    "    \n",
    "    x3 = mult * len(newrevsentenceslist)\n",
    "    \n",
    "    #Adding s to indicate sentences in index like s1, s2..\n",
    "    #Adding identifier for two texts' sentences t1s1, t1s2, t2s1..\n",
    "    indexnew = range(1,len(newrevsentenceslist)+1)\n",
    "    indexnew = [mystring + str(x) for x in indexnew]    \n",
    "\n",
    "    #creating a numpy array of numbers to allow decimal points\n",
    "    ylist = np.arange(0.95, 0, -0.05) \n",
    "    #Trimming the lists based on number of sentences\n",
    "    y3 = ylist[:len(newrevsentenceslist)]\n",
    "\n",
    "    #Creating the combined list of all x-positions for the csv \n",
    "    #Converting numpy arrays to python lists to create the y list\n",
    "    y = y3.tolist()\n",
    "    y = [ '%.2f' % elem for elem in y ] #to convert to 2 decimal points\n",
    "    \n",
    "    #Creating a temporary dataframe with all columns\n",
    "    tempdf = pd.DataFrame(\n",
    "        {'x': x3,'y': y,'index': indexnew,'sentence': newrevsentenceslist,'label':labelmoves}\n",
    "    )\n",
    "    #print(tempdf)\n",
    "    \n",
    "    newdf =  pd.concat([nodedf, tempdf], join='outer')\n",
    "    print(newdf)\n",
    "    return(newdf)\n",
    "\n",
    "#newdf = create_nodes_more(nodes_df, revsentenceslistnew, labelmovesnew, icount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create the dataframe for EDGES csv with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edges(origsentenceslist, revsentenceslist):\n",
    "    myedgesdf = pd.DataFrame(columns=['Start','End','Weight']) #Edges dataframe\n",
    "    templist = []\n",
    "    for i in range(0,len(origsentenceslist)):\n",
    "        for j in range(0,len(revsentenceslist)):\n",
    "            #Calculating cosine similarity between individual sentences\n",
    "            csvalue = calc_cosine(text_to_vector(origsentenceslist[i]), text_to_vector(revsentenceslist[j]))\n",
    "            num1 = i+1\n",
    "            num2 = j+1\n",
    "            myedge1 = \"t1s\" + str(num1)\n",
    "            myedge2 = \"t2s\" + str(num2)\n",
    "            #print(myedge1, myedge2, csvalue)\n",
    "            \n",
    "            #thresholds set based on available data - to be validated\n",
    "            #threshold changed from 1 since a sentence gave 0.99 similarity for the same sentence\n",
    "            if(csvalue >= 0.98):\n",
    "                templist.append([myedge1, myedge2, 1])\n",
    "                \n",
    "            if(csvalue > 0.8 and csvalue < 0.98):\n",
    "                templist.append([myedge1, myedge2, 0.8])\n",
    "                \n",
    "            if(csvalue > 0.6 and csvalue < 0.8):\n",
    "                templist.append([myedge1, myedge2, 0.6])\n",
    "                  \n",
    "    \n",
    "    #If no edges >0.6, then the list will be empty, create empty dataframe\n",
    "    if not templist:\n",
    "        tempdf = myedgesdf\n",
    "    else:\n",
    "        tempdf = pd.DataFrame(templist)  \n",
    "    \n",
    "    return(tempdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edges_more(origsentenceslist, revsentenceslist, icount):\n",
    "    myedgesdf = pd.DataFrame(columns=['Start','End','Weight']) #Edges dataframe\n",
    "    templist = []\n",
    "\n",
    "    if(icount==3):\n",
    "        edgeindex1 = \"t2s\"\n",
    "        edgeindex2 = \"t3s\"\n",
    "    elif(icount==4):\n",
    "        edgeindex1 = \"t3s\"\n",
    "        edgeindex2 = \"t4s\"\n",
    "    elif(icount==5):\n",
    "        edgeindex1 = \"t4s\"\n",
    "        edgeindex2 = \"t5s\"\n",
    "    elif(icount==6):\n",
    "        edgeindex1 = \"t5s\"\n",
    "        edgeindex2 = \"t6s\"\n",
    "    elif(icount==7):\n",
    "        edgeindex1 = \"t6s\"\n",
    "        edgeindex2 = \"t7s\"\n",
    " \n",
    "    for i in range(0,len(origsentenceslist)):\n",
    "        for j in range(0,len(revsentenceslist)):\n",
    "            #Calculating cosine similarity between individual sentences\n",
    "            csvalue = calc_cosine(text_to_vector(origsentenceslist[i]), text_to_vector(revsentenceslist[j]))\n",
    "            num1 = i+1\n",
    "            num2 = j+1\n",
    "            myedge1 = edgeindex1 + str(num1)\n",
    "            myedge2 = edgeindex2 + str(num2)\n",
    "            print(myedge1, myedge2, csvalue)\n",
    "            \n",
    "            #thresholds set based on available data - to be validated\n",
    "            #threshold changed from 1 since a sentence gave 0.99 similarity for the same sentence\n",
    "            if(csvalue >= 0.98):\n",
    "                templist.append([myedge1, myedge2, 1])\n",
    "                #tempdf = pd.DataFrame(templist)\n",
    "                \n",
    "            if(csvalue > 0.8 and csvalue < 0.98):\n",
    "                templist.append([myedge1, myedge2, 0.8])\n",
    "                #tempdf = pd.DataFrame(templist)\n",
    "                \n",
    "            if(csvalue > 0.6 and csvalue < 0.8):\n",
    "                templist.append([myedge1, myedge2, 0.6])\n",
    "                  \n",
    "    \n",
    "    #If no edges >0.6, then the list will be empty, create empty dataframe\n",
    "    if not templist:\n",
    "        tempdf = myedgesdf\n",
    "    else:\n",
    "        tempdf = pd.DataFrame(templist)  \n",
    "        #tempdf.columns = ['Start','End','Weight']\n",
    "    return(tempdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the given original essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = \"origtext.html\"\n",
    "orig_html = codecs.open(url1).read()\n",
    "\n",
    "soup1 = BeautifulSoup(orig_html, \"html.parser\")\n",
    "\n",
    "#kill all script and style elements\n",
    "for script in soup1([\"script\", \"style\"]):\n",
    "    script.extract()    # rip it out\n",
    "\n",
    "# get text\n",
    "origtxt = soup1.get_text()\n",
    "print(origtxt)\n",
    "print(\"Original essay length:\")\n",
    "print(len(origtxt))\n",
    "\n",
    "origtxt = origtxt.lower()\n",
    "origtxt = cleantext(origtxt)\n",
    "\n",
    "#Converting to vector to calculate overall cosine similarity\n",
    "vector1 = text_to_vector(origtxt)\n",
    "\n",
    "#Get sentences list from TAP\n",
    "origsentenceslist = markupMoveSentences(origtxt)\n",
    "origlen = len(origsentenceslist )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating analytics for the original given essay - rhetorical moves from TAP and creating nodes for the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moves1 = []\n",
    "for sent in origsentenceslist:\n",
    "    movesJson = findMoves(sent)\n",
    "    #print(movesJson)\n",
    "    try:\n",
    "        moves1.append(movesJson[0])\n",
    "    except IndexError:\n",
    "        moves1.append(movesJson)\n",
    "\n",
    "#Creating the list of rhetorical move labels for each sentence to add to nodes csv\n",
    "labelmoves1 = createlabels(moves1)\n",
    "print(labelmoves1)\n",
    "print(len(labelmoves1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create basic graph with one level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create the basic graph with one level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_graph(nodesurl,edgesurl):\n",
    "    ## Creating nodes and edges to render the graph\n",
    "    nodes_df = pd.read_csv(nodesurl, engine='python') #included engine for unicode error\n",
    "    edges_df = pd.read_csv(edgesurl, engine='python')\n",
    "    my_nodes = hv.Nodes(nodes_df).sort()\n",
    "\n",
    "    #Rendering the revision graph\n",
    "    mygraph = hv.Graph((edges_df, my_nodes), label=\"Revision Graph of Given vs Revised text (Sentence level comparison)\")\n",
    "    colors = ['#000000']+hv.Cycle('Category20').values\n",
    "    mygraph = mygraph.redim.range(x=(-0.05, 1.05), \n",
    "                                    y=(-0.05, 1.05) #original: y=(-0.05, 1.05)\n",
    "                                   ).options(color_index='label',edge_color_index='Weight', width=800, height=800, show_frame=True,\n",
    "                                             xaxis=None, yaxis=None,node_size=20, edge_line_width=1, cmap=['tan', 'blue','green'], edge_cmap='viridis')\n",
    "    return(mygraph) #to display the graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(url, origsentenceslist, labelmoves1):\n",
    "    #url = \"C:/Users/12696377/PycharmProjects/RevisionGraph/revhtmlfiles/1_RevisedEssay.html\"\n",
    "    rev_html = codecs.open(url).read()\n",
    "    soup2 = BeautifulSoup(rev_html, \"html.parser\")\n",
    "\n",
    "    #kill all script and style elements\n",
    "    for script in soup2([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    revtxt = soup2.get_text()\n",
    "    \n",
    "    print(\"Text Details:\\n\")\n",
    "\n",
    "    #Clean the text\n",
    "    revtxt = revtxt.lower()\n",
    "    revtxt = cleantext(revtxt)\n",
    "    print(\"Revised Text:\\n\")\n",
    "    print(revtxt)\n",
    "    print(\"Revised essay length:\")\n",
    "    revtxtlen = len(revtxt)\n",
    "    print(revtxtlen)\n",
    "\n",
    "    txt = url\n",
    "    filecount = txt.split('_')[0]\n",
    "    print(filecount)\n",
    "\n",
    "    #id = txt[txt.find('/')+len('/'):txt.rfind('_')]\n",
    "    #print(id)\n",
    "    \n",
    "    #Calculating overall cosine similarity score between original and revised texts\n",
    "    vector2 = text_to_vector(revtxt)\n",
    "    overallcosine = calc_cosine(vector1, vector2)\n",
    "    print('Cosine: %f' % overallcosine)\n",
    "\n",
    "    #Getting the list of sentences in the text from TAP\n",
    "    revsentenceslist = markupMoveSentences(revtxt)\n",
    "    #print(revsentenceslist)\n",
    "    revsents = len(revsentenceslist)\n",
    "    #print(revsents)\n",
    "\n",
    "    print(\"Number of words in the text:\")\n",
    "    revwords = len(revtxt.split())\n",
    "    #print(revwords)\n",
    "    \n",
    "    #Getting rhetorical moves for all sentences in the revised text\n",
    "    print(\"Rhetorical moves in the text:\\n\")\n",
    "    moves2 = []\n",
    "    for sent in revsentenceslist:\n",
    "        movesJson = findMoves(sent)\n",
    "        print(movesJson)\n",
    "        try:\n",
    "            #append the first element of list eg from [['tempstat', 'old']], get ['tempstat', 'old'] only (so we can get [[], ['tempstat', 'old'], ['emph', 'attitude'], ['contribution'], [], [], [], [], [], [], [], []] instead of [[[]], [['tempstat', 'old']], [['emph', 'attitude']], [['contribution']], [[]], [[]], [[]], [[]], [[]], [[]], [], [[]]])\n",
    "            moves2.append(movesJson[0])\n",
    "        except IndexError:\n",
    "            moves2.append(movesJson)\n",
    "\n",
    "\n",
    "    #Creating the list of rhetorical move labels for each sentence to add to nodes csv\n",
    "    labelmoves2 = createlabels(moves2)\n",
    "    print(labelmoves2)\n",
    "    labelmoves = labelmoves1 + labelmoves2\n",
    "    \n",
    "    #Call function to create the nodels csv\n",
    "    create_nodes(origsentenceslist,revsentenceslist,labelmoves)\n",
    "\n",
    "    #Calling function to create edges dataframe which includes edges for cohesive sentences inside the same text\n",
    "    df = create_edges(origsentenceslist,revsentenceslist)\n",
    "    df.columns= ['Start','End','Weight']\n",
    "    print(df)\n",
    "\n",
    "    #Writing to edges csv\n",
    "    df.to_csv(\"data/myedgesfinal.csv\", encoding='utf-8', index=False)\n",
    "    print(\"Edges csv created\")\n",
    "    \n",
    "    return(revsentenceslist) #For next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = 'inputfiles'\n",
    "filelist = os.listdir(path)\n",
    "print(filelist)\n",
    "\n",
    "filelist = [i for i in filelist if 'html' in i] #filter and only use files ending with html\n",
    "#print(filelist[0])\n",
    "#print(filelist[len(filelist)-1])\n",
    "print(len(filelist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Level 1 Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filelist[0])\n",
    "filepath = \"inputfiles/\"\n",
    "url = filepath + filelist[0]\n",
    "revsentenceslistprior = create_graph(url, origsentenceslist, labelmoves1)\n",
    "\n",
    "nodesurl = 'data/mynodesfinal.csv'\n",
    "edgesurl = 'data/myedgesfinal.csv'\n",
    "\n",
    "render_graph(nodesurl, edgesurl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to add multiple levels to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_graph_multi(url, origsentenceslist, icount, nodesurl, edgesurl):\n",
    "    rev_html = codecs.open(url).read()\n",
    "    soup2 = BeautifulSoup(rev_html, \"html.parser\")\n",
    "\n",
    "    #kill all script and style elements\n",
    "    for script in soup2([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    revtxt = soup2.get_text()\n",
    "    \n",
    "    print(\"Text Details:\\n\")\n",
    "\n",
    "    #Clean the text\n",
    "    revtxt = revtxt.lower()\n",
    "    revtxt = cleantext(revtxt)\n",
    "    print(\"Revised Text:\\n\")\n",
    "    print(revtxt)\n",
    "    print(\"Revised essay length:\")\n",
    "    revtxtlen = len(revtxt)\n",
    "    print(revtxtlen)\n",
    "\n",
    "    txt = url\n",
    "    filecount = txt.split('_')[0]\n",
    "    print(filecount)\n",
    "\n",
    "    #id = txt[txt.find('/')+len('/'):txt.rfind('_')]\n",
    "    #print(id)\n",
    "    \n",
    "    #Calculating overall cosine similarity score between original and revised texts\n",
    "    vector2 = text_to_vector(revtxt)\n",
    "    overallcosine = calc_cosine(vector1, vector2)\n",
    "    print('Cosine: %f' % overallcosine)\n",
    "\n",
    "    #Getting the list of sentences in the text from TAP\n",
    "    revsentenceslist = markupMoveSentences(revtxt)\n",
    "    #print(revsentenceslist)\n",
    "    revsents = len(revsentenceslist)\n",
    "    #print(revsents)\n",
    "\n",
    "    print(\"Number of words in the text:\")\n",
    "    revwords = len(revtxt.split())\n",
    "    #print(revwords)\n",
    "    \n",
    "    #Getting rhetorical moves for all sentences in the revised text\n",
    "    #print(\"Rhetorical moves in the text:\\n\")\n",
    "    moves2 = []\n",
    "    for sent in revsentenceslist:\n",
    "        movesJson = findMoves(sent)\n",
    "        print(movesJson)\n",
    "        try:\n",
    "            #append the first element of list eg from [['tempstat', 'old']], get ['tempstat', 'old'] only (so we can get [[], ['tempstat', 'old'], ['emph', 'attitude'], ['contribution'], [], [], [], [], [], [], [], []] instead of [[[]], [['tempstat', 'old']], [['emph', 'attitude']], [['contribution']], [[]], [[]], [[]], [[]], [[]], [[]], [], [[]]])\n",
    "            moves2.append(movesJson[0])\n",
    "        except IndexError:\n",
    "            moves2.append(movesJson)\n",
    "\n",
    "\n",
    "    #Creating the list of rhetorical move labels for each sentence to add to nodes csv\n",
    "    labelmoves = createlabels(moves2)\n",
    "    #print(labelmoves2)\n",
    "    #labelmoves = labelmoves1 + labelmoves2\n",
    "    \n",
    "    nodes_df = pd.read_csv(nodesurl, engine='python')\n",
    "    edges_df = pd.read_csv(edgesurl, engine='python')\n",
    "    \n",
    "    #Call function to create the nodels csv\n",
    "    newdf = create_nodes_more(nodes_df, revsentenceslist, labelmoves, icount)\n",
    "    newdf.to_csv(nodesurl, encoding = 'utf-8', index=False)\n",
    "    print(\"Nodes csv updated\")\n",
    "\n",
    "    #Calling function to create edges dataframe which includes edges for cohesive sentences inside the same text\n",
    "    df = create_edges_more(origsentenceslist,revsentenceslist, icount)\n",
    "    df.columns= ['Start','End','Weight']\n",
    "    newedgesdf =  pd.concat([edges_df, df], join='outer')\n",
    "    newedgesdf.to_csv(edgesurl, encoding='utf-8', index=False)\n",
    "    print(\"Edges csv updated\")\n",
    "    \n",
    "    #Return the list of sentences from the current revised text \n",
    "    return(revsentenceslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1 #-1 to be done\n",
    "while(i < len(filelist)):\n",
    "    print(filelist[i])\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i= 1 #i=2 left out t3 2 #-1 to be done, since level 1 graph is already built\n",
    "icount = 3 #icount = 4 left out t3\n",
    "while(i < len(filelist)):\n",
    "    url = filepath + filelist[i]\n",
    "    revsentenceslistout = add_graph_multi(url, revsentenceslistprior, icount, nodesurl, edgesurl)\n",
    "    revsentenceslistprior = revsentenceslistout\n",
    "    render_graph(nodesurl, edgesurl)\n",
    "    i = i+1\n",
    "    icount = icount+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodesurl = 'data/mynodesfinal.csv'\n",
    "edgesurl = 'data/myedgesfinal.csv'\n",
    "mygraph = render_graph(nodesurl, edgesurl)\n",
    "\n",
    "mygraph\n",
    "mygraph.relabel(\"Stages in revision from given to revised text (Sentence level comparison)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer = hv.renderer('bokeh')\n",
    "renderer.save(mygraph, 'MultistageRevisionGraph') #Saves a html file of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image('graphdescription.png', width=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
