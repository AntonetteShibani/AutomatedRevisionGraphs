{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Revision Graph\n",
    "A simple revision graph visualizing differences between two given texts at a sentence level.\n",
    "\n",
    "**Copyright (C) 2020 Antonette Shibani.** Available under the Creative Commons Attribution 3.0 Unported License (https://creativecommons.org/licenses/by/3.0/).\n",
    "\n",
    "Cite work as: _Antonette Shibani (2020) Constructing Automated Revision Graphs: A novel visualization technique to study student writing. In proceedings of the 21th International Conference on Artificial Intelligence in Education._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input files: \n",
    "\"origtext.html\" (Text1), \"revtext.html\" (Text2)\n",
    "#### Output file: \n",
    "\"SimpleRevisionGraph.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML  # Allows us to create annotated text using HTML and CSS\n",
    "import tabulate as tb\n",
    "from IPython.display import Image\n",
    "import csv\n",
    "try:\n",
    "    # Python 2\n",
    "    from itertools import izip\n",
    "except ImportError:\n",
    "    # Python 3\n",
    "    izip = zip\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import holoviews as hv \n",
    "import networkx as nx\n",
    "hv.extension('bokeh')\n",
    "#Interactive Graphs: http://holoviews.org/user_guide/Network_Graphs.html\n",
    "\n",
    "%opts Graph [width=400 height=400] \n",
    "\n",
    "import json                                 # We need to be able to work whith JSON\n",
    "from urllib import request, response        # To create requests to TAP and handle responses from TAP API\n",
    "#https://github.com/heta-io/tap\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions required for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate cosine similarity between two texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def calc_cosine(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to clean the text by removing characters leading to wrong sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleantext(txt):\n",
    "    txt.lower()\n",
    "    txt.replace('?.', '?')\n",
    "    txt.replace('\\xa0', '\\n\\n')\n",
    "    txt = re.sub(r'\\.+', \".\", txt) #regular expression to replace more than one full stop with one full stop\n",
    "    return txt   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAP setup to get analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tapUrl = \"https://tap.utscic.edu.au/\"       # TAP URL\n",
    "endpoint = \"graphql\"                        # The query endpoint on TAP\n",
    "completeUrl = tapUrl + endpoint             # The complete url that the request is posted to\n",
    "#Note: For the open-source version of Text Analytics Pipeline (TAP) API, see https://github.com/heta-io/tap\n",
    "\n",
    "#Connect to TAP to get metrics in the form of json output\n",
    "def getJsonFromTAP(query,text,url):\n",
    "    variables = {'text': text}\n",
    "    escapedQuery = query.replace(\"\\n\", \"\\\\n\") #query.encode('utf8').decode('unicode_escape')\n",
    "    fullQuery = json.dumps({'query': escapedQuery, 'variables': variables})\n",
    "    jsonHeader = {'Content-Type':'application/json'}\n",
    "    tapReq = request.Request(url, data = fullQuery.encode('utf8'), headers = jsonHeader)\n",
    "    tapResponse = \"\"\n",
    "    try:\n",
    "        tapResponse = request.urlopen(tapReq)\n",
    "        body = tapResponse.read().decode('utf8')           \n",
    "        return json.loads(body)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return json.dumps({})\n",
    "    \n",
    "    \n",
    "#Get sentences of text from TAP\n",
    "def markupMoveSentences(para):\n",
    "    #Get all sentences from the given text\n",
    "    sentencesQuery = \"query Sentences($text: String!){ annotations(text: $text) { analytics {original} }}\"\n",
    "    jsonData = getJsonFromTAP(sentencesQuery,para,completeUrl)\n",
    "    sentencesJson = jsonData.get('data').get('annotations').get('analytics')\n",
    "    def getSentence(json):\n",
    "        return (json.get('original'))\n",
    "    sentences = list(map(getSentence,sentencesJson)) #get original sentences from text\n",
    "    return sentences\n",
    "\n",
    "#Rhetorical moves parsing from TAP\n",
    "def findMoves(rawtext):\n",
    "    movesQuery =  \"query RhetoricalMoves($text: String) { moves(text:$text,parameters:\\\"{\\\\\\\"grammar\\\\\\\":\\\\\\\"analytic\\\\\\\"}\\\") {analytics}}\"\n",
    "    jsonData = getJsonFromTAP(movesQuery,rawtext,completeUrl)\n",
    "    moves = jsonData.get('data').get('moves').get('analytics')\n",
    "    return(moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create the list of rhetorical move labels for each sentence to add to nodes csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createlabels(moves):\n",
    "    rhetmoveslist = ['emph','contribution','novstat','contrast','tempstat','Surprise','nostat','grow','attitude']\n",
    "    movecount = []\n",
    "    label = []\n",
    "    for i in moves:\n",
    "        if len(i)==0:\n",
    "            label.append('Zero')\n",
    "        else:\n",
    "            #Calculate intersection of moves with the given list of valid moves\n",
    "            intmoves = list(set(i) & set(rhetmoveslist))\n",
    "            if len(intmoves)==1:\n",
    "                label.append('One')\n",
    "            if len(intmoves)==2:\n",
    "                label.append('Two')\n",
    "            if len(intmoves)>2:\n",
    "                label.append('More')\n",
    "    return(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create the NODES csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nodes(origsentenceslist, revsentenceslist, labelmoves):\n",
    "    #Creating the combined list of all sentences for the csv\n",
    "    sentencelist = origsentenceslist+revsentenceslist\n",
    "    \n",
    "    #create-list-with-numbers-between-2-values\n",
    "    index1 = range(1,len(origsentenceslist)+1)\n",
    "    index2 = range(1,len(revsentenceslist)+1)\n",
    "\n",
    "    #Adding s to indicate sentences in index like s1, s2..\n",
    "    #Adding identifier for two texts' sentences t1s1, t1s2, t2s1..\n",
    "    mystring1 = \"t1s\" #appending-the-same-string-to-a-list-of-strings-in-python\n",
    "    mystring2 = \"t2s\"\n",
    "    index1 = [mystring1 + str(x) for x in index1]\n",
    "    index2 = [mystring2 + str(x) for x in index2]\n",
    "\n",
    "    #Creating the combined list of all indices for the csv\n",
    "    indexlist = index1 + index2\n",
    "\n",
    "    x1 = [0.1] * len(origsentenceslist)\n",
    "    x2 = [0.5] * len(revsentenceslist)\n",
    "    #Creating the combined list of all x-positions for the csv\n",
    "    x=x1+x2\n",
    "    print(x)\n",
    "\n",
    "    #creating a numpy array of numbers to allow decimal points\n",
    "    ylist = np.arange(0.95, 0, -0.05) \n",
    "    #Trimming the lists based on number of sentences\n",
    "    y1 = ylist[:len(origsentenceslist)]\n",
    "    y2 = ylist[:len(revsentenceslist)]\n",
    "\n",
    "    #Creating the combined list of all x-positions for the csv \n",
    "    #Converting numpy arrays to python lists to create the y list\n",
    "    y = y1.tolist() + y2.tolist()\n",
    "    y = [ '%.2f' % elem for elem in y ] #to convert to 2 decimal points\n",
    "    #print(y)\n",
    "    \n",
    "    #Note: create 'data' folder in the directory\n",
    "    #write-data-from-two-lists-into-columns-in-a-csv\n",
    "    with open('data/mynodesfinal.csv', 'w', newline='') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(['x','y','index','sentence','label'])\n",
    "        w.writerows(zip(x,y,indexlist,sentencelist,labelmoves))\n",
    "    print(\"Nodes csv created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create the dataframe for EDGES csv with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edges(origsentenceslist, revsentenceslist):\n",
    "    myedgesdf = pd.DataFrame(columns=['Start','End','Weight']) #Edges dataframe\n",
    "    templist = []\n",
    "    for i in range(0,len(origsentenceslist)):\n",
    "        for j in range(0,len(revsentenceslist)):\n",
    "            #Calculating cosine similarity between individual sentences\n",
    "            csvalue = calc_cosine(text_to_vector(origsentenceslist[i]), text_to_vector(revsentenceslist[j]))\n",
    "            num1 = i+1\n",
    "            num2 = j+1\n",
    "            myedge1 = \"t1s\" + str(num1)\n",
    "            myedge2 = \"t2s\" + str(num2)\n",
    "            #print(myedge1, myedge2, csvalue)\n",
    "            \n",
    "            #threshold changed from 1 since a sentence gave 0.99 similarity for the same sent\n",
    "            if(csvalue >= 0.98):\n",
    "                templist.append([myedge1, myedge2, 1])\n",
    "                #tempdf = pd.DataFrame(templist)\n",
    "                \n",
    "            if(csvalue > 0.8 and csvalue < 0.98):\n",
    "                templist.append([myedge1, myedge2, 0.8])\n",
    "                #tempdf = pd.DataFrame(templist)\n",
    "                \n",
    "            if(csvalue > 0.6 and csvalue < 0.8):\n",
    "                templist.append([myedge1, myedge2, 0.6])\n",
    "                  \n",
    "    \n",
    "    #If no edges >0.6, then the list will be empty, create empty dataframe\n",
    "    if not templist:\n",
    "        tempdf = myedgesdf\n",
    "    else:\n",
    "        tempdf = pd.DataFrame(templist)  \n",
    "    \n",
    "    return(tempdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the given original essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#directory/ system path of text1\n",
    "url1 = \"origtext.html\"\n",
    "orig_html = codecs.open(url1).read()\n",
    "\n",
    "soup1 = BeautifulSoup(orig_html, \"html.parser\")\n",
    "\n",
    "#kill all script and style elements\n",
    "for script in soup1([\"script\", \"style\"]):\n",
    "    script.extract()    # rip it out\n",
    "\n",
    "# get text and clean it\n",
    "origtxt = soup1.get_text()\n",
    "origtxt = origtxt.lower()\n",
    "origtxt = cleantext(origtxt)\n",
    "print(\"Original Text:\\n\")\n",
    "print(origtxt)\n",
    "print(len(origtxt))\n",
    "\n",
    "#Converting to vector to calculate overall cosine similarity\n",
    "vector1 = text_to_vector(origtxt)\n",
    "\n",
    "#Get sentences list from TAP\n",
    "origsentenceslist = markupMoveSentences(origtxt)\n",
    "origlen = len(origsentenceslist )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating analytics for the original given essay - rhetorical moves from TAP and creating nodes for the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moves1 = []\n",
    "for sent in origsentenceslist:\n",
    "    movesJson = findMoves(sent)\n",
    "    #print(movesJson)\n",
    "    try:\n",
    "        moves1.append(movesJson[0])\n",
    "    except IndexError:\n",
    "        moves1.append(movesJson)\n",
    "\n",
    "#Creating the list of rhetorical move labels for each sentence to add to nodes csv\n",
    "labelmoves1 = createlabels(moves1)\n",
    "print(labelmoves1)\n",
    "print(len(labelmoves1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the revised text for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory/ system path of text2\n",
    "url2 = \"revtext.html\"\n",
    "\n",
    "rev_html = codecs.open(url2).read()\n",
    "\n",
    "soup2 = BeautifulSoup(rev_html, \"html.parser\")\n",
    "\n",
    "#kill all script and style elements\n",
    "for script in soup2([\"script\", \"style\"]):\n",
    "    script.extract()    # rip it out\n",
    "\n",
    "# get text\n",
    "revtxt = soup2.get_text()\n",
    "#print(revtxt)\n",
    "    \n",
    "#Clean the text\n",
    "revtxt = revtxt.lower()\n",
    "revtxt = cleantext(revtxt)\n",
    "print(\"Revised Text:\\n\")\n",
    "print(revtxt)\n",
    "print(\"Revised essay length:\")\n",
    "revtxtlen = len(revtxt)\n",
    "print(revtxtlen)\n",
    "\n",
    "txt = url2\n",
    "filecount = txt.split('_')[0]\n",
    "\n",
    "id = txt[txt.find('/')+len('/'):txt.rfind('_')]\n",
    "print(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the revision graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating overall cosine similarity score between original and revised texts\n",
    "vector2 = text_to_vector(revtxt)\n",
    "overallcosine = calc_cosine(vector1, vector2)\n",
    "print('Overall Cosine at document level: %f' % overallcosine)\n",
    "\n",
    "#Getting the list of sentences in the text from TAP\n",
    "revsentenceslist = markupMoveSentences(revtxt)\n",
    "revsents = len(revsentenceslist)\n",
    "revwords = len(revtxt.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple revision graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Getting rhetorical moves for all sentences in the revised text\n",
    "moves2 = []\n",
    "for sent in revsentenceslist:\n",
    "    movesJson = findMoves(sent)\n",
    "    try:\n",
    "        #append the first element of list eg from [['tempstat', 'old']], get ['tempstat', 'old'] only (so we can get [[], ['tempstat', 'old'], ['emph', 'attitude'], ['contribution'], [], [], [], [], [], [], [], []] instead of [[[]], [['tempstat', 'old']], [['emph', 'attitude']], [['contribution']], [[]], [[]], [[]], [[]], [[]], [[]], [], [[]]])\n",
    "        moves2.append(movesJson[0])\n",
    "    except IndexError:\n",
    "        moves2.append(movesJson)\n",
    "\n",
    "#print(moves2)\n",
    "\n",
    "#Creating the list of rhetorical move labels for each sentence to add to nodes csv\n",
    "labelmoves2 = createlabels(moves2)\n",
    "labelmoves = labelmoves1 + labelmoves2\n",
    "\n",
    "#Call function to create the nodels csv\n",
    "create_nodes(origsentenceslist,revsentenceslist,labelmoves)\n",
    "\n",
    "#Calling function to create edges dataframe which includes edges for cohesive sentences inside the same text\n",
    "df = create_edges(origsentenceslist,revsentenceslist)\n",
    "df.columns= ['Start','End','Weight']\n",
    "print(df)\n",
    "\n",
    "#Writing to edges csv\n",
    "df.to_csv(\"data/myedgesfinal.csv\", encoding='utf-8', index=False)\n",
    "print(\"Edges csv created\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Creating nodes and edges to render the graph\n",
    "nodes_df = pd.read_csv('data/mynodesfinal.csv', engine='python') #included engine for unicode error\n",
    "#nodes_df = pd.read_csv('Newnodes_try.csv', engine='python') \n",
    "edges_df = pd.read_csv('data/myedgesfinal.csv', engine='python')\n",
    "my_nodes = hv.Nodes(nodes_df).sort()\n",
    "\n",
    "\n",
    "#Rendering the revision graph\n",
    "mygraph = hv.Graph((edges_df, my_nodes), label='Simple Revision Graph')\n",
    "colors = ['#000000']+hv.Cycle('Category20').values\n",
    "mygraph = mygraph.redim.range(x=(-0.05, 1.05), \n",
    "                                y=(-0.05, 1.05)\n",
    "                               ).options(color_index='label',edge_color_index='Weight', width=800, height=800, show_frame=True,\n",
    "                                         xaxis=None, yaxis=None,node_size=20, edge_line_width=1, cmap=['tan', 'blue','green'], edge_cmap='viridis')\n",
    "#cmap = colors \n",
    "mygraph\n",
    "mygraph.relabel(\"Revision Graph of Given vs Revised text (Sentence level comparison)\") \n",
    "\n",
    "\n",
    "#renderer = hv.renderer('bokeh')\n",
    "#renderer.save(mygraph, figurl) #Saves a html file of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the graph  - creating filename for the graph html\n",
    "figurl = 'SimpleRevisionGraph'\n",
    "\n",
    "renderer = hv.renderer('bokeh')\n",
    "renderer.save(mygraph,figurl) #Saves a html file of the graph #'simplerevgraph') #Saves a html file of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating summary counts from the graph - optional\n",
    "def calc_metrics(nodes,edges):\n",
    "    bufferlist = []\n",
    "    \n",
    "    nodes_df_new1 = nodes_df[nodes_df['index'].str.contains(\"t1\")] #Only count rhetorical moves from text1(original)\n",
    "    rcountt1 = nodes_df_new1.set_index([\"x\",\"y\",\"sentence\",\"label\"]).count(level=\"label\")\n",
    "    print(\"Count of rhetorical move categories in sentences of text1:\")\n",
    "    print(rcountt1)\n",
    "    \n",
    "    nodes_df_new2 = nodes_df[nodes_df['index'].str.contains(\"t2\")] #Only count rhetorical moves from text2(revised)\n",
    "    rcountt2 = nodes_df_new2.set_index([\"x\",\"y\",\"sentence\",\"label\"]).count(level=\"label\")\n",
    "    print(\"Count of rhetorical move categories in sentences of text2:\")\n",
    "    print(rcountt2)\n",
    "    \n",
    "    #Counting the number of weighted edges\n",
    "    print(\"Count of the similarity score levels based on edges:\")\n",
    "    edgeweights = edges_df.set_index([\"Start\",\"Weight\"]).count(level=\"Weight\")\n",
    "    print(edgeweights)\n",
    "    \n",
    "    #Counting number of rhetorical moves from nodes\n",
    "    zerocount = len(nodes_df_new2[nodes_df_new2.label == 'Zero'])\n",
    "    onecount = len(nodes_df_new2[nodes_df_new2.label == 'One'])\n",
    "    twocount = len(nodes_df_new2[nodes_df_new2.label == 'Two'])\n",
    "    morecount = len(nodes_df_new2[nodes_df_new2.label == 'More'])\n",
    "\n",
    "    #Counting number of edges\n",
    "    medcount = len(edges_df[edges_df.Weight == 0.6])\n",
    "    highcount = len(edges_df[edges_df.Weight == 0.8])\n",
    "    samecount = len(edges_df[edges_df.Weight == 1.0])\n",
    "    \n",
    "    bufferlist.append([revtxtlen, overallcosine, revsents, revwords, \n",
    "                       zerocount, onecount, twocount, morecount, medcount, highcount, samecount])\n",
    "    summarydf = pd.DataFrame(bufferlist)\n",
    "    summarydf.columns = ['revtxtlen','cosinesim','revsents','revwords',\n",
    "                                      'zeronodes','onenodes','twonodes','morenodes',\n",
    "                                     'mededges','highedges','sameedges']\n",
    "\n",
    "    print(summarydf)\n",
    "    \n",
    "#Display graph metrics\n",
    "calc_metrics(nodes_df,edges_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
